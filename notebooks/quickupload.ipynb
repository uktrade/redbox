{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from uuid import UUID, uuid4\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from mypy_boto3_s3.client import S3Client\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Iterable\n",
    "import jsonlines\n",
    "import json\n",
    "import datetime\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "from django.core.files.uploadedfile import SimpleUploadedFile\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan, bulk\n",
    "\n",
    "from redbox.models import Settings, File\n",
    "from redbox.models.settings import ElasticLocalSettings\n",
    "from redbox.loader import UnstructuredDocumentLoader\n",
    "from redbox.embeddings import get_embeddings\n",
    "\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain.globals import set_verbose\n",
    "from langchain_elasticsearch.vectorstores import ElasticsearchStore\n",
    "\n",
    "from dj_notebook import activate, Plus\n",
    "\n",
    "ROOT = Path().resolve().parent\n",
    "\n",
    "set_verbose(False)\n",
    "\n",
    "_ = load_dotenv(find_dotenv(ROOT / \".env\"))\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "ENV = Settings(\n",
    "    minio_host=\"localhost\",\n",
    "    object_store=\"minio\",\n",
    "    elastic=ElasticLocalSettings(host=\"localhost\"),\n",
    ")\n",
    "MODEL = ENV.embedding_model\n",
    "\n",
    "S3_CLIENT = ENV.s3_client()\n",
    "ES_CLIENT = ENV.elasticsearch_client()\n",
    "\n",
    "EMBEDDING_MODEL = get_embeddings(ENV)\n",
    "VECTOR_STORE = ElasticsearchStore(\n",
    "    index_name=\"redbox-data-chunk\",\n",
    "    embedding=EMBEDDING_MODEL,\n",
    "    es_connection=ES_CLIENT,\n",
    "    query_field=\"text\",\n",
    "    vector_query_field=ENV.embedding_document_field_name,\n",
    ")\n",
    "\n",
    "try:\n",
    "    S3_CLIENT.create_bucket(\n",
    "        Bucket=ENV.bucket_name,\n",
    "        CreateBucketConfiguration={\"LocationConstraint\": ENV.aws_region},\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] != \"BucketAlreadyOwnedByYou\":\n",
    "        raise\n",
    "\n",
    "sys.path.insert(0, str(ROOT / \"django_app\"))\n",
    "\n",
    "RB_APP = activate(dotenv_file=str(ROOT / \"django_app/.env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick upload\n",
    "\n",
    "The worker takes forever locally. This notebook will chunk and upload stuff using your local machine, which is much quicker for me. Unlike the eval notebooks, this also makes entried in the Postgres database.\n",
    "\n",
    "This notebook needs the following services running:\n",
    "\n",
    "```\n",
    "docker compose up core-api db -d\n",
    "```\n",
    "\n",
    "It's also important that both `.env` files contain the same embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get your user UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RB_APP.read_frame(RB_APP.User.objects.all())[[\"id\", \"email\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_UUID = RB_APP.User.objects.first().id  # or read it and hardcode\n",
    "USER_UUID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider clearing all files in Elastic and Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RB_APP.read_frame(RB_APP.File.objects.all())[[\"id\", \"core_file_uuid\", \"original_file_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_index(index: str, es: Elasticsearch) -> None:\n",
    "    documents = scan(es, index=index, query={\"query\": {\"match_all\": {}}})\n",
    "    bulk_data = [{\"_op_type\": \"delete\", \"_index\": doc[\"_index\"], \"_id\": doc[\"_id\"]} for doc in documents]\n",
    "    bulk(es, bulk_data, request_timeout=300)\n",
    "\n",
    "\n",
    "def clear_bucket(bucket: str, s3: S3Client) -> None:\n",
    "    response = s3.list_objects_v2(Bucket=bucket)\n",
    "    if \"Contents\" in response:\n",
    "        # Delete each object\n",
    "        for obj in response[\"Contents\"]:\n",
    "            s3.delete_object(Bucket=bucket, Key=obj[\"Key\"])\n",
    "\n",
    "\n",
    "clear_index(index=\"redbox-data-chunk\", es=ES_CLIENT)\n",
    "clear_index(index=\"redbox-data-file\", es=ES_CLIENT)\n",
    "_ = RB_APP.File.objects.all().delete()\n",
    "clear_bucket(bucket=\"redbox-storage-dev\", s3=S3_CLIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bucket_objects(bucket: str, s3: S3Client = S3_CLIENT) -> int:\n",
    "    object_count = 0\n",
    "    response = s3.list_objects_v2(Bucket=bucket)\n",
    "\n",
    "    if \"Contents\" in response:\n",
    "        object_count += len(response[\"Contents\"])\n",
    "        # Paginate\n",
    "        while response[\"IsTruncated\"]:\n",
    "            continuation_token = response[\"NextContinuationToken\"]\n",
    "            response = s3.list_objects_v2(Bucket=bucket, ContinuationToken=continuation_token)\n",
    "            object_count += len(response[\"Contents\"])\n",
    "\n",
    "    return object_count\n",
    "\n",
    "\n",
    "def count_uploads(es: Elasticsearch = ES_CLIENT, dj_shell: Plus = RB_APP, s3: S3Client = S3_CLIENT):\n",
    "    return {\n",
    "        \"django_files\": dj_shell.File.objects.count(),\n",
    "        \"s3_files\": count_bucket_objects(bucket=\"redbox-storage-dev\", s3=s3),\n",
    "        \"elastic_files\": es.count(index=\"redbox-data-file\", body={\"query\": {\"match_all\": {}}})[\"count\"],\n",
    "        \"elastic_chunks\": es.count(index=\"redbox-data-chunk\", body={\"query\": {\"match_all\": {}}})[\"count\"],\n",
    "    }\n",
    "\n",
    "\n",
    "count_uploads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we embed and upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_django(\n",
    "    file_path: Path,\n",
    "    user_uuid: UUID = USER_UUID,\n",
    "    dj_shell: Plus = RB_APP,\n",
    "):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        file = dj_shell.File.objects.create(\n",
    "            status=\"complete\",\n",
    "            original_file=SimpleUploadedFile(name=file_path.name, content=f.read()),\n",
    "            user=RB_APP.User.objects.get(id=user_uuid),\n",
    "            original_file_name=file_path.name,\n",
    "            core_file_uuid=uuid4(),\n",
    "        )\n",
    "        file.save()\n",
    "\n",
    "    return file\n",
    "\n",
    "\n",
    "def embed_and_upload_file(\n",
    "    file_path: Path,\n",
    "    user_uuid: UUID = USER_UUID,\n",
    "    s3_client: S3Client = S3_CLIENT,\n",
    "    vector_store: VectorStore = VECTOR_STORE,\n",
    "    dj_shell: Plus = RB_APP,\n",
    ") -> None:\n",
    "    print(f\"Processing {file_path.name}\")\n",
    "\n",
    "    # Add to Django\n",
    "    dj_file = add_to_django(file_path=file_path, user_uuid=user_uuid, dj_shell=dj_shell)\n",
    "\n",
    "    es_file = File(\n",
    "        uuid=dj_file.core_file_uuid,\n",
    "        key=dj_file.url.parts[-1],\n",
    "        bucket=dj_file.url.parts[1],\n",
    "        creator_user_uuid=user_uuid,\n",
    "    )\n",
    "\n",
    "    # Add to S3\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        s3_client.upload_fileobj(f, \"redbox-storage-dev\", dj_file.url.parts[-1])\n",
    "\n",
    "    print(f\"Added {file_path.name} to S3 and Django\")\n",
    "\n",
    "    # Chunk\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        loader = UnstructuredDocumentLoader(file=es_file, file_bytes=f, env=ENV)\n",
    "\n",
    "        chunks = list(loader.lazy_load())\n",
    "\n",
    "    print(f\"Chunked {file_path.name} ({len(chunks)} chunks)\")\n",
    "\n",
    "    # Embed and upload\n",
    "    vector_store.add_documents(chunks)\n",
    "\n",
    "    print(f\"{file_path.name} complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/Demo Data/Energy\")\n",
    "# DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/D&D\")\n",
    "# DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/Lit\")\n",
    "# DIR_TO_UPLOAD: Path = Path(\"/Users/willlangdale/Downloads/DS\")\n",
    "\n",
    "for file_path in DIR_TO_UPLOAD.rglob(\"[!.]*.*\"):\n",
    "    embed_and_upload_file(\n",
    "        file_path=file_path,\n",
    "        user_uuid=USER_UUID,\n",
    "        s3_client=S3_CLIENT,\n",
    "        vector_store=VECTOR_STORE,\n",
    "        dj_shell=RB_APP,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it uploaded okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_uploads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can bring up the Django app and use the files.\n",
    "\n",
    "```\n",
    "docker compose up django-app -d --wait\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Embed and save\n",
    "\n",
    "While this is done elsewhere I found it helpful to keep here. Here we save the embeddings to `.jsonl` with a random file UUID without adding them to Django."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Path(\"/Users/willlangdale/Downloads/Demo Data/Energy\") / \"CS013b_Energy stats monthly brief september 2022.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in test.glob(\"*.*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test.open(\"rb\") as f:\n",
    "    es_file = File(\n",
    "        uuid=uuid4(),\n",
    "        key=\"\",\n",
    "        bucket=\"\",\n",
    "        creator_user_uuid=test_user,\n",
    "    )\n",
    "\n",
    "    loader = UnstructuredDocumentLoader(file=es_file, file_bytes=f, env=ENV)\n",
    "\n",
    "    chunks = list(loader.lazy_load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_embedded: list[dict] = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i > 2:\n",
    "        break\n",
    "    chunk_dict = chunk.dict()\n",
    "    chunk_dict[\"text\"] = chunk.page_content\n",
    "    chunk_dict[\"embedding\"] = EMBEDDING_MODEL.embed_documents([chunk.page_content])[0]\n",
    "\n",
    "    del chunk_dict[\"page_content\"]\n",
    "    del chunk_dict[\"type\"]\n",
    "    del chunk_dict[\"id\"]\n",
    "\n",
    "    chunks_embedded.append(chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_embedded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_embedded_dict(\n",
    "    file_path: Path,\n",
    "    user_uuid: UUID = USER_UUID,\n",
    ") -> None:\n",
    "    \"\"\"Embeds a file as a list of document dictionaries.\n",
    "\n",
    "    Aims to replicate the shape the vector store adds, but randomises the UUID\n",
    "    instead of using Django's.\n",
    "\n",
    "    Converts UUIDs to strings ASAP for serialisation simplicity.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {file_path.name}\")\n",
    "\n",
    "    es_file = File(\n",
    "        uuid=str(uuid4()),\n",
    "        key=\"\",\n",
    "        bucket=\"\",\n",
    "        creator_user_uuid=str(user_uuid),\n",
    "    )\n",
    "\n",
    "    # Chunk\n",
    "    with file_path.open(\"rb\") as f:\n",
    "        loader = UnstructuredDocumentLoader(file=es_file, file_bytes=f, env=ENV)\n",
    "\n",
    "        chunks = list(loader.lazy_load())\n",
    "\n",
    "    print(f\"Chunked {file_path.name} ({len(chunks)} chunks)\")\n",
    "\n",
    "    # Embed\n",
    "    chunks_embedded: list[dict] = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk_dict = chunk.dict()\n",
    "        chunk_dict[\"text\"] = chunk.page_content\n",
    "        chunk_dict[\"embedding\"] = EMBEDDING_MODEL.embed_documents([chunk.page_content])[0]\n",
    "\n",
    "        del chunk_dict[\"page_content\"]\n",
    "        del chunk_dict[\"type\"]\n",
    "        del chunk_dict[\"id\"]\n",
    "\n",
    "        chunks_embedded.append(chunk_dict)\n",
    "\n",
    "    print(f\"Embedded {file_path.name} ({len(chunks_embedded)} chunks)\")\n",
    "\n",
    "    return chunks_embedded\n",
    "\n",
    "\n",
    "def save_doc_dicts_to_jsonl(documents: Iterable[dict], file_path: str) -> None:\n",
    "    class DocumentJSONEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, datetime.datetime):\n",
    "                return obj.isoformat()\n",
    "            if isinstance(obj, UUID):\n",
    "                return str(obj)\n",
    "            return super().default(obj)\n",
    "\n",
    "    def doc_dumps(obj: dict):\n",
    "        return json.dumps(obj, cls=DocumentJSONEncoder)\n",
    "\n",
    "    with jsonlines.open(file_path, mode=\"w\", dumps=doc_dumps) as writer:\n",
    "        writer.write_all(documents)\n",
    "\n",
    "\n",
    "def embed_dir(dir_path: Path, out_file: Path, user_uuid: UUID = USER_UUID) -> None:\n",
    "    print(f\"Embedding {dir_path}\")\n",
    "    chunks: list[dict] = []\n",
    "    for file_path in dir_path.rglob(\"[!.]*.*\"):\n",
    "        chunks += file_to_embedded_dict(\n",
    "            file_path=file_path,\n",
    "            user_uuid=user_uuid,\n",
    "        )\n",
    "\n",
    "    save_doc_dicts_to_jsonl(documents=chunks, file_path=out_file)\n",
    "    print(f\"Saved {dir_path} to {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_embedded[0]\n",
    "save_doc_dicts_to_jsonl(chunks_embedded, Path(\"test.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_test = file_to_embedded_dict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TO_SAVE: Path = Path(\"/Users/willlangdale/Downloads/Demo Data/Energy\")\n",
    "# DIR_TO_SAVE: Path = Path(\"/Users/willlangdale/Downloads/D&D\")\n",
    "# DIR_TO_SAVE: Path = Path(\"/Users/willlangdale/Downloads/Lit\")\n",
    "# DIR_TO_SAVE: Path = Path(\"/Users/willlangdale/Downloads/DS\")\n",
    "\n",
    "OUT_DIR: Path = DIR_TO_SAVE / \"text-embedding-3-large.jsonl\"\n",
    "\n",
    "embed_dir(dir_path=DIR_TO_SAVE, out_file=OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-Vh_-Fb0j-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
