{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisor agent (tutorial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses supervisor agent implementation from Langgraph. It calls either Researcher (to return web search results) or Coder (to perform calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "# members = [\"researcher\", \"coder\"]\n",
    "members = [\"researcher\", \"coder\"]\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = members + [\"FINISH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a supervisor agent responsible for coordinating between a researcher agent and a coder agent. \n",
    "Given the following user request, respond with the agent to act next. Each agent will perform a task and respond with their results and status. When finished, respond with FINISH.\n",
    "\n",
    "You should follow these steps:\n",
    "\n",
    "1. Analyze user queries as well as conversation history and determine if agent(s) should handle the request\n",
    "2. Write out your reasoning and analysis of how to route this request and determine which agent(s) should handle the request\n",
    "3. Route requests to the appropriate agent(s)\n",
    "4. Evaluate whether previous message fully answers the user query, without explicitely providing instructions to find the results.\n",
    "5. If the user query was fully answered without explicitely providing instructions to find the results:\n",
    "     return: FINISH\n",
    "6. If user query was not fully answered, repeat previous steps. \n",
    "\n",
    "Guidelines for request handling:\n",
    "\n",
    "1. if the request requires web search:\n",
    "   - return: researcher \n",
    "\n",
    "2. if the request requires performing mathematical operations:\n",
    "   - return: coder\n",
    "\n",
    "3. if the request has a complete answer in the conversation history without explicitely providing instructions to find the results:\n",
    "   - return: FINISH\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(model=\"anthropic.claude-3-sonnet-20240229-v1:0\", model_provider=\"bedrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    next: str\n",
    "\n",
    "\n",
    "def supervisor_node(state: State) -> Command[Literal[*members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)  # llm_openai give better response\n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto, update={\"next\": goto})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINISH: Returns the final response to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    return (\n",
    "        \"Here are the different headcounts for each of the FAANG companies in 2024:\\n\"\n",
    "        \"1. **Facebook (Meta)**: 67,317 employees.\\n\"\n",
    "        \"2. **Apple**: 164,000 employees.\\n\"\n",
    "        \"3. **Amazon**: 1,551,000 employees.\\n\"\n",
    "        \"4. **Netflix**: 14,000 employees.\\n\"\n",
    "        \"5. **Google (Alphabet)**: 181,269 employees.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PROMPT = \"\"\"You are a researcher. Your task is to search the web using the available tools.\n",
    "<guidelines>\n",
    "Only return the results from the web search\n",
    "Do not alter or summarise the responses returned by the web search\n",
    "Do not perform mathematical operations\n",
    "</guidelines>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESEARCH_PROMPT = \"\"\"You are a researcher. Your task is to search the web using the available tools.\n",
    "\n",
    "Guidelines for request handling:\n",
    "\n",
    "1. Do not perform mathematical operations\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PROMPT = \"\"\"You are a coder. Your task is to perform mathematical operations using the available tools.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "research_agent = create_react_agent(llm, tools=[web_search], prompt=RESEARCH_PROMPT, name=\"researcher\")\n",
    "\n",
    "\n",
    "def research_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")]},\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
    "code_agent = create_react_agent(llm, tools=[add, multiply], prompt=CODE_PROMPT, name=\"coder\")\n",
    "\n",
    "\n",
    "def code_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = code_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")]},\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_node(\"coder\", code_node)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's the combined headcount of the FAANG companies in 2024?\")]}, subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
