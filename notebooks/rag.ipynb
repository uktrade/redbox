{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_elasticsearch import ApproxRetrievalStrategy, ElasticsearchStore\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.settings import ElasticLocalSettings\n",
    "from redbox.storage import ElasticsearchStorageHandler\n",
    "from redbox.transform import bedrock_tokeniser\n",
    "\n",
    "from core_api.callbacks import LoggerCallbackHandler\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "ROOT = Path().resolve().parent\n",
    "\n",
    "_ = load_dotenv(find_dotenv(ROOT / \".env\"))\n",
    "\n",
    "logging.basicConfig(steam=sys.stdout, level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "env = Settings(\n",
    "    _env_file=(ROOT / \".env\"),\n",
    "    minio_host=\"localhost\",\n",
    "    object_store=\"minio\",\n",
    "    elastic=ElasticLocalSettings(host=\"localhost\"),\n",
    ")\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=env.embedding_model, cache_folder=\"../models/\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\n",
    "        {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": env.elastic.port,\n",
    "            \"scheme\": env.elastic.scheme,\n",
    "        }\n",
    "    ],\n",
    "    basic_auth=(env.elastic.user, env.elastic.password),\n",
    ")\n",
    "\n",
    "if env.elastic.subscription_level == \"basic\":\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=False)\n",
    "elif env.elastic.subscription_level in [\"platinum\", \"enterprise\"]:\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=True)\n",
    "\n",
    "vector_store = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=\"redbox-data-chunk\",\n",
    "    embedding=embedding_model,\n",
    "    strategy=strategy,\n",
    "    vector_query_field=\"embedding\",\n",
    ")\n",
    "\n",
    "# See core_api.dependecies for details on this hack\n",
    "os.environ[\"AZURE_API_VERSION\"] = env.openai_api_version\n",
    "\n",
    "logger_callback = LoggerCallbackHandler(logger=log)\n",
    "\n",
    "llm = ChatLiteLLM(\n",
    "    model=env.azure_openai_model,\n",
    "    streaming=True,\n",
    "    azure_key=env.azure_openai_api_key,\n",
    "    api_base=env.azure_openai_endpoint,\n",
    "    max_tokens=1_024,\n",
    "    callbacks=[logger_callback],\n",
    ")\n",
    "\n",
    "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index)\n",
    "\n",
    "tokeniser = bedrock_tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_api.retriever import ParameterisedElasticsearchRetriever\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "\n",
    "def get_parameterised_retriever(env, es):\n",
    "    \"\"\"Creates an Elasticsearch retriever runnable.\n",
    "\n",
    "    Runnable takes input of a dict keyed to question, file_uuids and user_uuid.\n",
    "\n",
    "    Runnable returns a list of Chunks.\n",
    "    \"\"\"\n",
    "    default_params = {\n",
    "        \"size\": env.ai.rag_k,\n",
    "        \"num_candidates\": env.ai.rag_num_candidates,\n",
    "        \"match_boost\": 1,\n",
    "        \"knn_boost\": 1,\n",
    "        \"similarity_threshold\": 0,\n",
    "    }\n",
    "    return ParameterisedElasticsearchRetriever(\n",
    "        es_client=es,\n",
    "        index_name=f\"{env.elastic_root_index}-chunk\",\n",
    "        params=default_params,\n",
    "        embedding_model=embedding_model,\n",
    "    ).configurable_fields(\n",
    "        params=ConfigurableField(\n",
    "            id=\"params\", name=\"Retriever parameters\", description=\"A dictionary of parameters to use for the retriever.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "retriever = get_parameterised_retriever(env, es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\n",
    "    input={\n",
    "        \"question\": \"KAN\",\n",
    "        \"file_uuids\": [\n",
    "            \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\",  # KAN paper\n",
    "            # \"1a9d18a7-9499-47b6-abcc-4e82370028ee\" # MAMBA paper\n",
    "        ],\n",
    "        \"user_uuid\": \"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain.schema import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from redbox.models import ChatRoute\n",
    "from redbox.models.chain import RedboxQuery\n",
    "\n",
    "from core_api.format import format_documents\n",
    "from core_api.runnables import make_chat_prompt_from_messages_runnable\n",
    "\n",
    "\n",
    "def build_retrieval_chain(\n",
    "    llm,\n",
    "    retriever,\n",
    "    tokeniser,\n",
    "    env,\n",
    ") -> Runnable:\n",
    "    return (\n",
    "        RunnablePassthrough.assign(documents=retriever)\n",
    "        | RunnablePassthrough.assign(\n",
    "            formatted_documents=(RunnablePassthrough() | itemgetter(\"documents\") | format_documents)\n",
    "        )\n",
    "        | {\n",
    "            \"response\": make_chat_prompt_from_messages_runnable(\n",
    "                system_prompt=env.ai.retrieval_system_prompt,\n",
    "                question_prompt=env.ai.retrieval_question_prompt,\n",
    "                input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "                tokeniser=tokeniser,\n",
    "            )\n",
    "            | llm\n",
    "            | StrOutputParser(),\n",
    "            \"source_documents\": itemgetter(\"documents\"),\n",
    "            \"route_name\": RunnableLambda(lambda _: ChatRoute.search.value),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "rag = build_retrieval_chain(llm, retriever, tokeniser, env)\n",
    "\n",
    "params = RedboxQuery(\n",
    "    question=\"Give the full citation.\",\n",
    "    file_uuids=[\n",
    "        \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\",  # KAN paper\n",
    "        \"1a9d18a7-9499-47b6-abcc-4e82370028ee\",  # MAMBA paper\n",
    "    ],\n",
    "    user_uuid=\"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
    "    chat_history=[\n",
    "        {\"text\": \"What is the fastest attention that the authors are aware of?\", \"role\": \"user\"},\n",
    "        {\n",
    "            \"text\": \"The fastest implementation of attention, according to the authors, is **FlashAttention-2 (Dao 2024)** with a causal mask. It's stated that this version of FlashAttention-2 is approximately **1.7× faster** than the version without a causal mask because roughly half of the attention entries are computed.\",\n",
    "            \"role\": \"ai\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "rag.invoke(params.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    ")\n",
    "from redbox.models.chain import RedboxQuery\n",
    "\n",
    "\n",
    "CONDENSE_SYSTEM_PROMPT = (\n",
    "    \"Given the following conversation and a follow up question, generate a follow \"\n",
    "    \"up question to be a standalone question. \"\n",
    "    \"You are only allowed to generate one question in response. \"\n",
    "    \"Include sources from the chat history in the standalone question created, \"\n",
    "    \"when they are available. \"\n",
    "    \"If you don't know the answer, just say that you don't know, \"\n",
    "    \"don't try to make up an answer. \\n\"\n",
    ")\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = \"{question}\\n=========\\n Standalone question: \"\n",
    "\n",
    "\n",
    "def build_condense_retrieval_chain(\n",
    "    llm,\n",
    "    retriever,\n",
    "    tokeniser,\n",
    "    env,\n",
    ") -> Runnable:\n",
    "    def route(input_dict: dict):\n",
    "        if len(input_dict[\"chat_history\"]) > 0:\n",
    "            return RunnablePassthrough.assign(\n",
    "                question=make_chat_prompt_from_messages_runnable(\n",
    "                    system_prompt=env.ai.condense_system_prompt,\n",
    "                    question_prompt=env.ai.condense_question_prompt,\n",
    "                    input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "                    tokeniser=tokeniser,\n",
    "                )\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            )\n",
    "        else:\n",
    "            return RunnablePassthrough()\n",
    "\n",
    "    return (\n",
    "        RunnableLambda(route)\n",
    "        | RunnablePassthrough.assign(documents=retriever)\n",
    "        | RunnablePassthrough.assign(\n",
    "            formatted_documents=(RunnablePassthrough() | itemgetter(\"documents\") | format_documents)\n",
    "        )\n",
    "        | {\n",
    "            \"response\": make_chat_prompt_from_messages_runnable(\n",
    "                system_prompt=env.ai.retrieval_system_prompt,\n",
    "                question_prompt=env.ai.retrieval_question_prompt,\n",
    "                input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "                tokeniser=tokeniser,\n",
    "            )\n",
    "            | llm\n",
    "            | StrOutputParser(),\n",
    "            \"source_documents\": itemgetter(\"documents\"),\n",
    "            \"route_name\": RunnableLambda(lambda _: ChatRoute.search.value),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# crag = make_chat_prompt_from_messages_runnable(\n",
    "#     system_prompt=CONDENSE_SYSTEM_PROMPT,\n",
    "#     question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "#     input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "#     tokeniser=tokeniser,\n",
    "# ) | llm\n",
    "\n",
    "crag = build_condense_retrieval_chain(llm, retriever, tokeniser, env)\n",
    "\n",
    "params = RedboxQuery(\n",
    "    question=\"Give the full citation.\",\n",
    "    file_uuids=[\n",
    "        \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\",  # KAN paper\n",
    "        \"1a9d18a7-9499-47b6-abcc-4e82370028ee\",  # MAMBA paper\n",
    "    ],\n",
    "    user_uuid=\"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
    "    chat_history=[\n",
    "        {\"text\": \"What is the fastest attention that the authors are aware of?\", \"role\": \"user\"},\n",
    "        {\n",
    "            \"text\": \"The fastest implementation of attention, according to the authors, is **FlashAttention-2 (Dao 2024)** with a causal mask. It's stated that this version of FlashAttention-2 is approximately **1.7× faster** than the version without a causal mask because roughly half of the attention entries are computed.\",\n",
    "            \"role\": \"ai\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "crag.invoke(params.model_dump())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-Vh_-Fb0j-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
