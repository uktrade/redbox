{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.settings import ElasticLocalSettings\n",
    "from redbox.storage import ElasticsearchStorageHandler\n",
    "from redbox.transform import bedrock_tokeniser\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "ROOT = Path().resolve().parent\n",
    "\n",
    "_ = load_dotenv(find_dotenv(ROOT / \".env\"))\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "env = Settings(\n",
    "    _env_file=(ROOT / \".env\"),\n",
    "    minio_host=\"localhost\",\n",
    "    object_store=\"minio\",\n",
    "    elastic=ElasticLocalSettings(host=\"localhost\"),\n",
    ")\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=env.embedding_model, cache_folder=\"../models/\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\n",
    "        {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": env.elastic.port,\n",
    "            \"scheme\": env.elastic.scheme,\n",
    "        }\n",
    "    ],\n",
    "    basic_auth=(env.elastic.user, env.elastic.password),\n",
    ")\n",
    "\n",
    "# See core_api.dependecies for details on this hack\n",
    "os.environ[\"AZURE_API_VERSION\"] = env.openai_api_version\n",
    "\n",
    "llm = ChatLiteLLM(\n",
    "    model=env.azure_openai_model,\n",
    "    streaming=True,\n",
    "    azure_key=env.azure_openai_api_key,\n",
    "    api_base=env.azure_openai_endpoint,\n",
    "    max_tokens=1_024,\n",
    ")\n",
    "\n",
    "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index)\n",
    "\n",
    "tokeniser = bedrock_tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarisation scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_api.retriever import ParameterisedElasticsearchRetriever\n",
    "\n",
    "retriever = ParameterisedElasticsearchRetriever(\n",
    "    es_client=es,\n",
    "    index_name=f\"{env.elastic_root_index}-chunk\",\n",
    "    embedding_model=embedding_model,\n",
    "    params={\n",
    "        \"size\": 1,\n",
    "        \"num_candidates\": 100,\n",
    "        \"match_boost\": 1,\n",
    "        \"knn_boost\": 2,\n",
    "        \"similarity_threshold\": 0.7,\n",
    "    },\n",
    ")\n",
    "\n",
    "retriever.invoke(\n",
    "    input={\n",
    "        \"question\": \"\",\n",
    "        \"file_uuids\": [\n",
    "            # \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\", # KAN paper\n",
    "            \"1a9d18a7-9499-47b6-abcc-4e82370028ee\"  # MAMBA paper,\n",
    "            # \"450a972c-356a-4fdb-b080-3af4fa9b0b74\", #backendnotes\n",
    "        ],\n",
    "        \"user_uuid\": \"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from elasticsearch.helpers import scan\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_elasticsearch import ElasticsearchRetriever\n",
    "\n",
    "# from core_api.retriever import get_all_chunks_query\n",
    "from core_api.retriever.base import ESQuery\n",
    "\n",
    "\n",
    "def get_all_chunks_query(query: ESQuery) -> dict[str, Any]:\n",
    "    query_filter = [\n",
    "        {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\"term\": {\"creator_user_uuid.keyword\": str(query[\"user_uuid\"])}},\n",
    "                    {\"term\": {\"metadata.creator_user_uuid.keyword\": str(query[\"user_uuid\"])}},\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    if len(query[\"file_uuids\"]) != 0:\n",
    "        query_filter.append(\n",
    "            {\n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\"terms\": {\"parent_file_uuid.keyword\": [str(uuid) for uuid in query[\"file_uuids\"]]}},\n",
    "                        {\"terms\": {\"metadata.parent_file_uuid.keyword\": [str(uuid) for uuid in query[\"file_uuids\"]]}},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    return {\n",
    "        \"_source\": {\"excludes\": [\"embedding\"]},\n",
    "        \"query\": {\"bool\": {\"must\": {\"match_all\": {}}, \"filter\": query_filter}},\n",
    "    }\n",
    "\n",
    "\n",
    "class AllElasticsearchRetriever(ElasticsearchRetriever):\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> list[Document]:\n",
    "        if not self.es_client or not self.document_mapper:\n",
    "            raise ValueError(\"faulty configuration\")  # should not happen\n",
    "\n",
    "        body = self.body_func(query)\n",
    "        results = list(scan(client=self.es_client, index=self.index_name, query=body, source=True))\n",
    "\n",
    "        results_documents = [\n",
    "            Document(page_content=hit[\"_source\"][\"text\"], metadata=hit[\"_source\"][\"metadata\"]) for hit in results\n",
    "        ]\n",
    "\n",
    "        return sorted(results_documents, key=lambda result: result.metadata[\"index\"])\n",
    "\n",
    "\n",
    "all_chunks_retriever = AllElasticsearchRetriever(\n",
    "    es_client=es, index_name=f\"{env.elastic_root_index}-chunk\", body_func=get_all_chunks_query, content_field=\"text\"\n",
    ")\n",
    "\n",
    "# docs = all_chunks_retriever.invoke(\n",
    "#     input={\n",
    "#         \"question\": \"\",\n",
    "#         \"file_uuids\": [\n",
    "#             # \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\", # KAN paper\n",
    "#             \"1a9d18a7-9499-47b6-abcc-4e82370028ee\" # MAMBA paper,\n",
    "#             # \"450a972c-356a-4fdb-b080-3af4fa9b0b74\", #backendnotes\n",
    "#         ],\n",
    "#         \"user_uuid\": \"5c37bf4c-002c-458d-9e68-03042f76a5b1\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# docs[:3]\n",
    "# [doc.metadata[\"index\"] for doc in docs]\n",
    "\n",
    "body = get_all_chunks_query(\n",
    "    {\n",
    "        \"question\": \"\",\n",
    "        \"file_uuids\": [\n",
    "            # \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\", # KAN paper\n",
    "            \"1a9d18a7-9499-47b6-abcc-4e82370028ee\"  # MAMBA paper,\n",
    "            # \"450a972c-356a-4fdb-b080-3af4fa9b0b74\", #backendnotes\n",
    "        ],\n",
    "        \"user_uuid\": \"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
    "    }\n",
    ")\n",
    "results = list(scan(client=es, index=f\"{env.elastic_root_index}-chunk\", query=body, source=True))\n",
    "\n",
    "[Document(page_content=hit[\"_source\"][\"text\"], metadata=hit[\"_source\"][\"metadata\"]) for hit in results[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    "    chain,\n",
    ")\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from redbox.models import ChatRoute\n",
    "from redbox.models.errors import AIError\n",
    "\n",
    "from core_api.runnables import (\n",
    "    make_chat_prompt_from_messages_runnable,\n",
    "    resize_documents,\n",
    ")\n",
    "\n",
    "\n",
    "def build_summary_chain(\n",
    "    llm,\n",
    "    all_chunks_retriever,\n",
    "    tokeniser,\n",
    "    env,\n",
    ") -> Runnable:\n",
    "    def make_document_context(input_dict: dict):\n",
    "        return (\n",
    "            all_chunks_retriever\n",
    "            | {\n",
    "                str(file_uuid): resize_documents(env.ai.summarisation_chunk_max_tokens)\n",
    "                for file_uuid in input_dict[\"file_uuids\"]\n",
    "            }\n",
    "            | RunnableLambda(lambda f: [chunk.page_content for chunk_lists in f.values() for chunk in chunk_lists])\n",
    "        ).invoke(input_dict)\n",
    "\n",
    "    # Stuff chain now missing the RunnabeLambda to format the chunks\n",
    "    stuff_chain = (\n",
    "        make_chat_prompt_from_messages_runnable(\n",
    "            system_prompt=env.ai.summarisation_system_prompt,\n",
    "            question_prompt=env.ai.summarisation_question_prompt,\n",
    "            input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "            tokeniser=tokeniser,\n",
    "        )\n",
    "        | llm\n",
    "        | {\n",
    "            \"response\": StrOutputParser(),\n",
    "            \"route_name\": RunnableLambda(lambda _: ChatRoute.stuff_summarise.value),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    @chain\n",
    "    def map_operation(input_dict):\n",
    "        system_map_prompt = env.ai.map_system_prompt\n",
    "        prompt_template = PromptTemplate.from_template(env.ai.chat_map_question_prompt)\n",
    "\n",
    "        formatted_map_question_prompt = prompt_template.format(question=input_dict[\"question\"])\n",
    "\n",
    "        map_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_map_prompt),\n",
    "                (\"human\", formatted_map_question_prompt + env.ai.map_document_prompt),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        documents = input_dict[\"documents\"]\n",
    "\n",
    "        map_summaries = (map_prompt | llm | StrOutputParser()).batch(\n",
    "            documents,\n",
    "            config=RunnableConfig(max_concurrency=env.ai.summarisation_max_concurrency),\n",
    "        )\n",
    "\n",
    "        summaries = \" ; \".join(map_summaries)\n",
    "        input_dict[\"summaries\"] = summaries\n",
    "        return input_dict\n",
    "\n",
    "    map_reduce_chain = (\n",
    "        map_operation\n",
    "        | make_chat_prompt_from_messages_runnable(\n",
    "            system_prompt=env.ai.reduce_system_prompt,\n",
    "            question_prompt=env.ai.reduce_question_prompt,\n",
    "            input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "            tokeniser=tokeniser,\n",
    "        )\n",
    "        | llm\n",
    "        | {\n",
    "            \"response\": StrOutputParser(),\n",
    "            \"route_name\": RunnableLambda(lambda _: ChatRoute.map_reduce_summarise.value),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    @chain\n",
    "    def summarisation_route(input_dict):\n",
    "        if len(input_dict[\"documents\"]) == 1:\n",
    "            return stuff_chain\n",
    "\n",
    "        elif len(input_dict[\"documents\"]) > 1:\n",
    "            return map_reduce_chain\n",
    "\n",
    "        else:\n",
    "            message = \"No documents to summarise\"\n",
    "            raise AIError(message)\n",
    "\n",
    "    return RunnablePassthrough.assign(documents=make_document_context) | summarisation_route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_chain = (\n",
    "    RunnablePassthrough.assign(documents=all_chunks_retriever)\n",
    "    | make_chat_prompt_from_messages_runnable(\n",
    "        system_prompt=env.ai.summarisation_system_prompt,\n",
    "        question_prompt=env.ai.summarisation_question_prompt,\n",
    "        input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "        tokeniser=tokeniser,\n",
    "    )\n",
    "    | llm\n",
    ")\n",
    "\n",
    "stuff_chain.invoke(\n",
    "    input={\n",
    "        \"question\": \"Summarise this paper\",\n",
    "        \"file_uuids\": [\n",
    "            # \"36ed2f1a-57a5-489c-a4cb-fbdd25e2b038\", # KAN paper\n",
    "            # \"1a9d18a7-9499-47b6-abcc-4e82370028ee\", # MAMBA paper\n",
    "            \"450a972c-356a-4fdb-b080-3af4fa9b0b74\",  # backend notes\n",
    "        ],\n",
    "        \"user_uuid\": \"5c37bf4c-002c-458d-9e68-03042f76a5b1\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-Vh_-Fb0j-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
