{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to investigate Plan-and-execute agentic pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan-And-Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based loosely on Wang, et. al.’s paper on Plan-and-Solve Prompting, and Yohei Nakajima’s BabyAGI project, this simple architecture is emblematic of the planning agent architecture. It consists of two basic components:\n",
    "\n",
    "1) A planner, which prompts an LLM to generate a multi-step plan to complete a large task.\n",
    "2) Executor(s), which accept the user query and a step in the plan and invoke 1 or more tools to complete that task.\n",
    "\n",
    "Once execution is completed, the agent is called again with a re-planning prompt, letting it decide whether to finish with a response or whether to generate a follow-up plan (if the first plan didn’t have the desired effect).\n",
    "\n",
    "This agent design lets us avoid having to call the large planner LLM for each tool invocation. It still is restricted by serial tool calling and uses an LLM for each task since it doesn't support variable assignment.\n",
    "\n",
    "There are 2 patterns that extends the plan-and-execute pattern:\n",
    "- Reasoning Without Observation (ReWOO) (Can execute multiple tasks without replaning)\n",
    "- LLM Compiler (Improve speed of task execution)\n",
    "\n",
    "source: https://blog.langchain.dev/planning-agents/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    "\n",
    "- Single task agent\n",
    "- Serial tool calling \n",
    "- Use LLM for each task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReWOO\n",
    "\n",
    "1) Planner generates a plan list consisting of interleaving \"Plan\" (reasoning) and \"E#\" lines.\n",
    "\n",
    "Query:  \"What are the stats for the quarterbacks of the super bowl contenders this year\"\n",
    "\n",
    "Plan: I need to know the teams playing in the superbowl this year\n",
    "E1: Search[Who is competing in the superbowl?]\n",
    "Plan: I need to know the quarterbacks for each team\n",
    "E2: LLM[Quarterback for the first team of #E1]\n",
    "\n",
    "The planner can reference previous outputs using syntax like #E2 . This means it can execute a task list without having to re-plan every time.\n",
    "\n",
    "2) The worker node loops through each task and assigns the task output to the corresponding variable. It also replaces variables with their results when calling subsequent calls.\n",
    "\n",
    "3) Finally, the Solver integrates all these outputs into a final answer.\n",
    "\n",
    "\n",
    "Pros:\n",
    "\n",
    "- This means it can execute a task list without having to re-plan every time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM Compiler\n",
    "\n",
    "The LLMCompiler has the following main components:\n",
    "\n",
    "1) Planner: streams a DAG of tasks. Each task contains a tool, arguments, and list of dependencies.\n",
    "\n",
    "2) Task Fetching Unit schedules and executes the tasks. This accepts a stream of tasks. This unit schedules tasks once their dependencies are met. Since many tools involve other calls to search engines or LLMs, the extra parallelism can grant a significant speed boost (the paper claims 3.6x).\n",
    "\n",
    "3) Joiner: dynamically replan or finish based on the entire graph history (including task execution results) is an LLM step that decides whether to respond with the final answer or whether to pass the progress back to the (re-)planning agent to continue work.\n",
    "\n",
    "The key runtime-boosting ideas here are:\n",
    "\n",
    "- Planner outputs are streamed; the output parser eagerly yields task parameters and their dependencies.\n",
    "\n",
    "- The task fetching unit receives the parsed task stream and schedules tasks once all their dependencies are satisfied.\n",
    "\n",
    "- Task arguments can be variables, which are the outputs of previous tasks in the DAG. For instance, the model can call search(\"${1}\") to search for queries generated by the output of task 1. This lets the agent work even faster than the \"embarrassingly parallel\" tool calling in OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv /redbox/.env.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.graph.nodes.tools import build_search_wikipedia_tool, build_search_documents_tool\n",
    "from redbox.models.settings import Settings\n",
    "from redbox.models.chain import RedboxQuery, RedboxState, AISettings, ChatLLMBackend\n",
    "from redbox.models.file import ChunkResolution\n",
    "from langchain_core.messages import AIMessage\n",
    "from redbox.chains.components import get_chat_llm\n",
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Settings()\n",
    "ai_claude_setting = AISettings(chat_backend=ChatLLMBackend(name=\"anthropic.claude-3-sonnet-20240229-v1:0\", provider=\"bedrock\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import Tool, tool\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from redbox.api.format import format_documents\n",
    "from redbox.chains.components import get_embeddings\n",
    "from redbox.models.chain import RedboxState\n",
    "from redbox.models.file import ChunkCreatorType, ChunkMetadata, ChunkResolution\n",
    "from redbox.models.settings import get_settings\n",
    "from redbox.transform import bedrock_tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_govuk_search_tool(filter=True) -> Tool:\n",
    "    \"\"\"Constructs a tool that searches gov.uk and sets state[\"documents\"].\"\"\"\n",
    "\n",
    "    tokeniser = bedrock_tokeniser\n",
    "\n",
    "    def recalculate_similarity(response, query, num_results):\n",
    "        embedding_model = get_embeddings(get_settings())\n",
    "        em_query = embedding_model.embed_query(query)\n",
    "        for r in response.get(\"results\"):\n",
    "            description = r.get(\"description\")\n",
    "            em_des = embedding_model.embed_query(description)\n",
    "            r[\"similarity\"] = cosine_similarity(np.array(em_query).reshape(1, -1), np.array(em_des).reshape(1, -1))[0][\n",
    "                0\n",
    "            ]\n",
    "        response[\"results\"] = sorted(response.get(\"results\"), key=lambda x: x[\"similarity\"], reverse=True)[:num_results]\n",
    "        return response\n",
    "\n",
    "    @tool(response_format=\"content_and_artifact\")\n",
    "    def _search_govuk(query: str) -> tuple[str, list[Document]]:\n",
    "        \"\"\"\n",
    "        Search for documents on gov.uk based on a query string.\n",
    "        This endpoint is used to search for documents on gov.uk. There are many types of documents on gov.uk.\n",
    "        Types include:\n",
    "        - guidance\n",
    "        - policy\n",
    "        - legislation\n",
    "        - news\n",
    "        - travel advice\n",
    "        - departmental reports\n",
    "        - statistics\n",
    "        - consultations\n",
    "        - appeals\n",
    "        \"\"\"\n",
    "        tool_govuk_retrieved_results = 10\n",
    "        tool_govuk_returned_results = 1\n",
    "        url_base = \"https://www.gov.uk\"\n",
    "        required_fields = [\n",
    "            \"format\",\n",
    "            \"title\",\n",
    "            \"description\",\n",
    "            \"indexable_content\",\n",
    "            \"link\",\n",
    "        ]\n",
    "        # ai_settings = state.request.ai_settings\n",
    "        response = requests.get(\n",
    "            f\"{url_base}/api/search.json\",\n",
    "            params={\n",
    "                \"q\": query,\n",
    "                \"count\": (\n",
    "                    tool_govuk_retrieved_results if filter else tool_govuk_returned_results\n",
    "                    # ai_settings.tool_govuk_retrieved_results if filter else ai_settings.tool_govuk_returned_results\n",
    "                ),\n",
    "                \"fields\": required_fields,\n",
    "            },\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        response = response.json()\n",
    "\n",
    "        if filter:\n",
    "            # response = recalculate_similarity(response, query, ai_settings.tool_govuk_returned_results)\n",
    "            response = recalculate_similarity(response, query, tool_govuk_returned_results)\n",
    "\n",
    "        mapped_documents = []\n",
    "        for i, doc in enumerate(response[\"results\"]):\n",
    "            if any(field not in doc for field in required_fields):\n",
    "                continue\n",
    "\n",
    "            mapped_documents.append(\n",
    "                Document(\n",
    "                    page_content=doc[\"indexable_content\"],\n",
    "                    metadata=ChunkMetadata(\n",
    "                        index=i,\n",
    "                        uri=f\"{url_base}{doc['link']}\",\n",
    "                        token_count=tokeniser(doc[\"indexable_content\"]),\n",
    "                        creator_type=ChunkCreatorType.gov_uk,\n",
    "                    ).model_dump(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return format_documents(mapped_documents), mapped_documents\n",
    "\n",
    "    return _search_govuk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "# Grabbing tools from redbox\n",
    "\n",
    "search_documents = build_search_documents_tool(\n",
    "    es_client=env.elasticsearch_client(),\n",
    "    index_name=env.elastic_chunk_alias,\n",
    "    embedding_model=env.embedding_backend,\n",
    "    embedding_field_name=env.embedding_document_field_name,\n",
    "    chunk_resolution=ChunkResolution.normal,\n",
    ")\n",
    "search_wikipedia = build_search_wikipedia_tool()\n",
    "search_govuk = build_govuk_search_tool()\n",
    "\n",
    "\n",
    "# tools = [search_documents, search_wikipedia, search_govuk]\n",
    "\n",
    "tools = [search_wikipedia, search_govuk]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Planner\n",
    "\n",
    "The planner accepts the input question and generates a task list to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_description(tools):\n",
    "    return \"\\n\".join(\n",
    "            f\"{i+1}. Name: {tool.name}. Description: {tool.description}.\\n\"\n",
    "            for i, tool in enumerate(\n",
    "                tools\n",
    "            )  # +1 to offset the 0 starting index, we want it count normally from 1.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAN_PROMPT = (' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "\"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "\" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "\" - You must continue the task index from the end of the previous one. Do not repeat task indices.\")\n",
    "\n",
    "PLAN_PROMPT = (\n",
    "    \"Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an\" \" action from the following {num_tools} types:\\n\"\n",
    "    \"{tool_descriptions}\\n\"\n",
    "    \"{num_tools}. join(): Collects and combines results from prior actions.\\n\\n\"\n",
    "\n",
    "    \"- An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are \"\n",
    "    \"executed.\"\n",
    "    \"- join should always be the last action in the plan, and will be called in two scenarios:\\n\"\n",
    "        \"(a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n\"\n",
    "    \"(b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n\"\n",
    "    \"- Each action described above contains input/output types and description.\\n\"\n",
    "    \" - You must strictly adhere to the input and output types for each action.\\n\"\n",
    "    \"- The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n\"\n",
    "    \"- Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n\"\n",
    "    \"- Each action MUST have a unique ID, which is strictly increasing.\\n\"\n",
    "    \"- Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the \" \"format $id to denote the ID of the previous action whose output will be the input.\\n\"\n",
    "    \"- Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n\"\n",
    "    \"- Ensure the plan maximizes parallelizability.\\n\"\n",
    "    \"- Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n\"\n",
    "    \"- Never introduce new actions other than the ones provided.\\n\\n\"\n",
    "    \n",
    "    \"{messages}\\n\\n\"\n",
    "    \n",
    "    \"Remember, ONLY respond with the task list in the correct format! E.g.:\\n\"\n",
    "    \"idx. tool(arg_name=args)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from output_parser import LLMCompilerPlanParser, Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = get_tool_description(tools)\n",
    "\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools)\n",
    "        + 1,  # Add one because we're adding the join() tool at the end.\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=REPLAN_PROMPT,\n",
    "        num_tools=len(tools) + 1,\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Task Fetching Unit\n",
    "\n",
    "This component schedules the tasks. It receives a stream of tools of the following format:\n",
    "\n",
    "{\n",
    "    tool: BaseTool,\n",
    "    dependencies: number[],\n",
    "}\n",
    "The basic idea is to begin executing tools as soon as their dependencies are met. This is done through multi-threading. We will combine the task fetching unit and executor below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from typing import Any, Dict, Iterable, List, Union\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    # $1 or ${1} -> 1\n",
    "    ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n",
    "\n",
    "    def replace_match(match):\n",
    "        # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n",
    "\n",
    "        # Return the match group, in this case the index, from the string. This is the index\n",
    "        # number we get back.\n",
    "        idx = int(match.group(1))\n",
    "        return str(observations.get(idx, match.group(0)))\n",
    "\n",
    "    # For dependencies on other tasks\n",
    "    if isinstance(arg, str):\n",
    "        return re.sub(ID_PATTERN, replace_match, arg)\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    args_for_tasks = {}\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            args_for_tasks[task[\"idx\"]] = task[\"args\"]\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke, dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], args_for_tasks[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(\n",
    "            name=name,\n",
    "            content=str(obs),\n",
    "            additional_kwargs={\"idx\": k, \"args\": task_args},\n",
    "            tool_call_id=k,\n",
    "        )\n",
    "        for k, (name, task_args, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # This is the primary \"agent\" in our application\n",
    "    planner_prompt = ChatPromptTemplate([(PLAN_PROMPT)])\n",
    "    planner = create_planner(get_chat_llm(ai_claude_setting.chat_backend), tools, planner_prompt)\n",
    "    tasks = planner.stream(messages)\n",
    "    # Begin executing the planner immediately\n",
    "    try:\n",
    "        tasks = itertools.chain([next(tasks)], tasks)\n",
    "    except StopIteration:\n",
    "        # Handle the case where tasks is empty.\n",
    "        tasks = iter([])\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if isinstance(state.get('counter', []), int):\n",
    "        print('There is counter')\n",
    "        counter = state.get('counter') + 1\n",
    "    else:\n",
    "        print('There is no counter')\n",
    "        counter = 0\n",
    "    \n",
    "    return {\"messages\": scheduled_tasks, 'counter':counter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Joiner\n",
    "\n",
    "So now we have the planning and initial execution done. We need a component to process these outputs and either:\n",
    "\n",
    "Respond with the correct answer.\n",
    "Loop with a new plan.\n",
    "The paper refers to this as the \"joiner\". It's another LLM call. We are using function calling to improve parsing reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINER_PROMPT = (\n",
    "    \"Solve a question answering task. Here are some guidelines:\\n\"\n",
    " \"- In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\\n\"\n",
    " \"- Thought needs to reason about the question based on the Observations in 1-2 sentences.\\n\"\n",
    " \"- Ignore irrelevant action results.\\n\"\n",
    " \"- If the required information is present, give a concise but complete and helpful answer to the user's question.\\n\"\n",
    " \"- If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the \" \"following format:\\n\"\n",
    "\n",
    "\"Thought: <reason about the task results and whether you have sufficient information to answer the question>\\n\"\n",
    "\"Action: <action to take>\\n\"\n",
    "\"Available actions:\\n\"\n",
    " \"(1) Finish(the final answer to return to the user): returns the answer and finishes the task.\\n\"\n",
    " \"(2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\\n\\n\"\n",
    "\"{messages}\\n\"\n",
    "\"Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\"\n",
    "\"If the data is incomplete or insufficient for a thorough response, your secondary role is to guide the user on how they can provide additional input or context to improve the outcome.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from redbox.chains.parser import ClaudeParser\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "parser = ClaudeParser(pydantic_object=JoinOutputs)\n",
    "joiner_prompt = ChatPromptTemplate(messages=[(JOINER_PROMPT + \"\\n\\n{format_instructions}\\n\")],\n",
    "                                   partial_variables = {\"format_instructions\": parser.get_format_instructions()})\n",
    "\n",
    "runnable = joiner_prompt | get_chat_llm(ai_claude_setting.chat_backend) | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return {\n",
    "            \"messages\": response\n",
    "            + [\n",
    "                SystemMessage(\n",
    "                    content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n",
    "\n",
    "\n",
    "def select_recent_messages(state) -> dict:\n",
    "    messages = state[\"messages\"]\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1], 'counter': state['counter']+1}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Create graph\n",
    "\n",
    "\n",
    "We'll define the agent as a stateful graph, with the main nodes being:\n",
    "\n",
    "Plan and execute (the DAG from the first step above)\n",
    "Join: determine if we should finish or replan\n",
    "Recontextualize: update the graph state based on the output from the joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    counter: int\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if state['counter'] > 3:\n",
    "        return END\n",
    "    if isinstance(messages[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "graph_builder.add_edge(START, \"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script to test performance of current Redbox and Plan-and-Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.app import Redbox\n",
    "\n",
    "def get_state(user_uuid, prompts, documents, ai_setting):\n",
    "    q = RedboxQuery(\n",
    "        question=f\"{prompts[-1]}\",\n",
    "        s3_keys=documents,\n",
    "        user_uuid=user_uuid,\n",
    "        chat_history=prompts[:-1],\n",
    "        ai_settings=ai_setting,\n",
    "        permitted_s3_keys=documents,\n",
    "    )\n",
    "\n",
    "    return RedboxState(\n",
    "        request=q,\n",
    "    )\n",
    "\n",
    "def run_app(app, state) -> RedboxState:\n",
    "    langfuse_handler = CallbackHandler()\n",
    "    return app.graph.invoke(state, config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "app = Redbox(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "def run_gadget(question):\n",
    "    x = get_state(uuid4(), prompts = [f'@gadget {question}'], documents = [], ai_setting = ai_claude_setting)\n",
    "    res = run_app(app, x)\n",
    "    return res['messages'][-1].content\n",
    "\n",
    "\n",
    "def run_plan_and_execute(question):\n",
    "    res = chain.invoke({\"messages\": [HumanMessage(content=f\"{question}\")]},{\"recursion_limit\": 10},)\n",
    "    return res['messages'][-1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save(filename, question, response, time_used, method):\n",
    "    df = pd.Series({'question': question,\n",
    "                            'response': response,\n",
    "                            'time': time_used,\n",
    "                            'method': method})\n",
    "    df.to_frame().T.to_csv(filename, mode='a', index=False, header=False)\n",
    "\n",
    "def exp(func_name, question):\n",
    "    print(f'question: {question}')\n",
    "    start = time.time()\n",
    "    response = eval(func_name)(question)\n",
    "    time_used = time.time() - start\n",
    "    return response, time_used\n",
    "\n",
    "def run_experiment(questions, methods, save_path):\n",
    "    for method in methods:\n",
    "        for i, question in enumerate(questions):\n",
    "            \n",
    "            if method == 'redbox':\n",
    "                response, time_used = exp(\"run_gadget\", question)\n",
    "            if method == 'plan_and_execute':\n",
    "                response, time_used = exp(\"run_plan_and_execute\", question)\n",
    "            print(response)\n",
    "            save(save_path, question, response, time_used, method)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['Who is UK PM?', \n",
    "             'What is the oldest parrot alive, and how much longer is that than the average?', \n",
    "             'How does the trend in birth affect the current tax system?',\n",
    "             'Compare the growth in tech businesses between UK and USA',\n",
    "             'List top 5 areas where AI can be used to save taxpayers money',\n",
    "             'What is the current state of UK economy? and how can it be improved?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(questions=questions,\n",
    "               methods=['plan_and_execute'],\n",
    "               save_path='exp_plan3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(questions=questions,\n",
    "               methods=['redbox'],\n",
    "               save_path='exp_plan.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-root-In7wI2Lt-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
