{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "\n",
    "%dotenv .env\n",
    "%dotenv -o ./.env.notebook\n",
    "\n",
    "# Notebook specific environment variables\n",
    "# import os\n",
    "# os.environ[\"COLLECTION_ENDPOINT\"] = \"http://admin:admin@localhost:9200\"\n",
    "# os.environ[\"MINIO_HOST\"] = \"localhost\"\n",
    "# os.environ[\"POSTGRES_HOST\"] = \"localhost\"\n",
    "# os.environ[\"UNSTRUCTURED_HOST\"] = \"localhost\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are investigating whether an LLM can identify correct tool e.g. search or summarise given user's question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore prompts that 1) uses only user'question to determine the route, and 2) uses user's question and basic documents metadata (document name, description and keywords) to determine route. The reason is that by giving information about document metadata, LLM is more equipped with information to help determining the route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Based on the experiment results, we achieved 80% using the prompt with metadata. In addition, using this prompt show that LLM uses slightly shorter time to make decision. Therefore, we will be implementing a new node with LLM_decide_tool_1 going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_decide_tool_0 = (\"\"\"Given analysis request, determine whether to use search or summarise tools.\n",
    "\n",
    "Context:\n",
    "- Search tool: Used to find and analyze specific relevant sections in a document\n",
    "- Summarise tool: Used to create an overview of the entire document's content\n",
    "\n",
    "Please analyze the following request:\n",
    "{question}\n",
    "\n",
    "Follow these steps to determine the appropriate tool:\n",
    "\n",
    "1. Identify the key requirements in the request:\n",
    "   - Is it asking for specific information or general overview?\n",
    "   - Are there specific topics/keywords mentioned?\n",
    "   - Is the scope focused or broad?\n",
    "\n",
    "2. Evaluate request characteristics:\n",
    "   - Does it need comprehensive coverage or targeted information?\n",
    "   - Are there specific questions to answer?\n",
    "   - Is context from the entire document needed?\n",
    "\n",
    "3. Recommend either search or summarise based on:\n",
    "   - If focused/specific information is needed → Recommend search\n",
    "   - If general overview/main points needed → Recommend summarise\n",
    "\n",
    "- Recommended Tool: [Search/Summarise]\n",
    "- Reason for the recommendation\n",
    "Provide your recommendation in this format:\n",
    "\\n{format_instructions}\\n\n",
    "                   \n",
    "Analysis request:\n",
    "{question}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_decide_tool_1 = (\"\"\"Given analysis request and document demtadata, determine whether to use search or summarise tools.\n",
    "\n",
    "Context:\n",
    "- Search tool: Used to find and analyze specific relevant sections in a document\n",
    "- Summarise tool: Used to create an overview of the entire document's content\n",
    "\n",
    "Please analyze the following request:\n",
    "{question}\n",
    "\n",
    "Follow these steps to determine the appropriate tool:\n",
    "\n",
    "1. Identify the key requirements in the request:\n",
    "   - Is it asking for specific information or general overview?\n",
    "   - Are there specific topics/keywords mentioned?\n",
    "   - Is the scope focused or broad?\n",
    "\n",
    "2. Evaluate request characteristics:\n",
    "   - Does it need comprehensive coverage or targeted information?\n",
    "   - Are there specific questions to answer?\n",
    "   - Is context from the entire document needed?\n",
    "\n",
    "3. Recommend either search or summarise based on:\n",
    "   - If focused/specific information is needed → Recommend search\n",
    "   - If general overview/main points needed → Recommend summarise\n",
    "   - Priortise search tool if both tools can be used to produce good answer \n",
    "\n",
    "- Recommended Tool: [Search/Summarise]\n",
    "\n",
    "Provide your recommendation in this format:\n",
    "\\n{format_instructions}\\n\n",
    "\n",
    "Analysis request:\n",
    "{question}\n",
    "                   \n",
    "Document metadata: {metadata}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating necceasry functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from redbox.chains.components import get_basic_metadata_retriever\n",
    "import time\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from redbox.chains.parser import ClaudeParser\n",
    "from pydantic import BaseModel\n",
    "from redbox.chains.components import get_chat_llm\n",
    "from redbox.models.chain import RedboxState, RedboxQuery, AISettings\n",
    "from langchain_core.runnables import chain\n",
    "from uuid import uuid4\n",
    "from redbox.models.settings import ChatLLMBackend\n",
    "from redbox.models.settings import get_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(user_uuid, prompts, documents, ai_setting):\n",
    "    q = RedboxQuery(\n",
    "        question=f\"{prompts[-1]}\",\n",
    "        s3_keys=documents,\n",
    "        user_uuid=user_uuid,\n",
    "        chat_history=prompts[:-1],\n",
    "        ai_settings=ai_setting,\n",
    "        permitted_s3_keys=documents,\n",
    "    )\n",
    "\n",
    "    return RedboxState(\n",
    "        request=q,\n",
    "    )\n",
    "\n",
    "def basic_chat_chain(system_prompt, _additional_variables: dict = {}, parser=None):\n",
    "    @chain\n",
    "    def _basic_chat_chain(state: RedboxState):\n",
    "        nonlocal parser\n",
    "        llm = get_chat_llm(state.request.ai_settings.chat_backend)\n",
    "        context = ({\n",
    "                    \"question\": state.request.question,\n",
    "                    }\n",
    "                | _additional_variables\n",
    "            )\n",
    "        \n",
    "        if parser:\n",
    "            format_instructions = parser.get_format_instructions()\n",
    "            prompt = ChatPromptTemplate([(system_prompt)], partial_variables={\"format_instructions\": format_instructions})\n",
    "        else:\n",
    "            prompt = ChatPromptTemplate([(system_prompt)])\n",
    "            parser = ClaudeParser()\n",
    "        chain = prompt | llm | parser\n",
    "        return chain.invoke(context)\n",
    "    return _basic_chat_chain\n",
    "\n",
    "def lm_choose_route(system_prompt: str, parser: ClaudeParser):\n",
    "    metadata = None\n",
    "    \n",
    "    @chain\n",
    "    def get_metadata(state: RedboxState):\n",
    "        nonlocal metadata\n",
    "        env = get_settings()\n",
    "        retriever = get_basic_metadata_retriever(env)\n",
    "        metadata = retriever.invoke(state)\n",
    "        return state\n",
    "    \n",
    "    @chain\n",
    "    def use_result(state: RedboxState):\n",
    "        chain = basic_chat_chain(system_prompt=system_prompt, parser=parser, _additional_variables={'metadata': metadata})\n",
    "        return chain.invoke(state)\n",
    "    \n",
    "    return get_metadata | use_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining class to capture LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"Search\", \"Summarise\"]\n",
    "\n",
    "#create options map for the supervisor output parser.\n",
    "tools_options = {tool:tool for tool in members}\n",
    "\n",
    "#create Enum object\n",
    "ToolEnum = Enum('ToolEnum', tools_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDecision(BaseModel):\n",
    "    next: ToolEnum = ToolEnum.Search\n",
    "\n",
    "class AgentDecisionWithReason(BaseModel):\n",
    "    next: ToolEnum = ToolEnum.Search\n",
    "    reason: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Redbox state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_state(user_uuid=uuid4(), prompts=['What did Serena Williams say about fairness in relation to her experience with postnatal complications?'], documents=['test@dbt.gov.uk/1 The power chapter.pdf'], ai_setting=AISettings(chat_backend=ChatLLMBackend(name=\"anthropic.claude-3-sonnet-20240229-v1:0\", provider=\"bedrock\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prompt 0 without metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = LLM_decide_tool_0\n",
    "agent_parser = ClaudeParser(pydantic_object=AgentDecisionWithReason)\n",
    "# return results with reason\n",
    "test_chain_reason = basic_chat_chain(system_prompt=prompt, parser=agent_parser)\n",
    "start = time.time()\n",
    "test_chain_reason.invoke(x)\n",
    "print(f'time used: {time.time() - start}')\n",
    "\n",
    "# return results without reason\n",
    "agent_parser = ClaudeParser(pydantic_object=AgentDecision)\n",
    "test_chain_no_reason = basic_chat_chain(system_prompt=prompt, parser=agent_parser)\n",
    "start = time.time()\n",
    "test_chain_no_reason.invoke(x)\n",
    "print(f'time used: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prompt 1 with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = LLM_decide_tool_1\n",
    "agent_parser = ClaudeParser(pydantic_object=AgentDecisionWithReason)\n",
    "# return results with reason\n",
    "test_chain_reason = lm_choose_route(system_prompt=prompt, parser=agent_parser)\n",
    "start = time.time()\n",
    "test_chain_reason.invoke(x)\n",
    "print(f'time used: {time.time() - start}')\n",
    "\n",
    "# return results without reason\n",
    "agent_parser = ClaudeParser(pydantic_object=AgentDecision)\n",
    "test_chain_no_reason = lm_choose_route(system_prompt=prompt, parser=agent_parser)\n",
    "start = time.time()\n",
    "test_chain_no_reason.invoke(x)\n",
    "print(f'time used: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiments\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./files/route_evaluation_consensus.xlsx - Sheet 1 - route_evaluation_new.csv\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making change on the chain and correct parser you want to test here\n",
    "agent_parser = ClaudeParser(pydantic_object=AgentDecisionWithReason)\n",
    "test_chain = lm_choose_route(system_prompt=LLM_decide_tool_1, parser=agent_parser)\n",
    "\n",
    "\n",
    "next_move = []\n",
    "move_reason = []\n",
    "for prompt in df.Prompt:\n",
    "    x = get_state(user_uuid=uuid4(), prompts=[prompt], documents=['test@dbt.gov.uk/1 The power chapter.pdf'], ai_setting=AISettings(chat_backend=ChatLLMBackend(name=\"anthropic.claude-3-sonnet-20240229-v1:0\", provider=\"bedrock\")))\n",
    "    res = test_chain.invoke(x)\n",
    "    next_move += [res.next.value]\n",
    "    move_reason += [res.reason] # comment this line out if you are not returning reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "save_path = \"./.intent_exports/intent_prompt3_basic_metadata.csv\"\n",
    "pd.DataFrame({'Prompt': df.Prompt, 'Consensus': df.consensus, 'LLM_tool_select': next_move, 'LLM_reason': move_reason}).to_csv(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
