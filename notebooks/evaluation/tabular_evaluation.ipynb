{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Download evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "url = \"https://bird-bench.oss-cn-beijing.aliyuncs.com/dev.zip\"\n",
    "downloaded_file = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp dev.zip ./data/tabular/dev.zip\n",
    "!rm -r dev.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"./data/tabular/dev.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data/tabular/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data/tabular/dev.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"./data/tabular/dev_20240627/dev_databases.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data/tabular/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data/tabular/dev_20240627/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your SQLite database file\n",
    "db_path = \"./data/tabular/dev_databases/financial/financial.sqlite\"\n",
    "# Directory where CSVs will be saved\n",
    "output_dir = \"./data/tabular/csv_tables\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "# Fetch all table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "# Export each table to CSV\n",
    "for table_name_tuple in tables:\n",
    "    table_name = table_name_tuple[0]\n",
    "    print(f\"Exporting table: {table_name}\")\n",
    "    # Quote the table name to handle reserved keywords\n",
    "    df = pd.read_sql_query(f'SELECT * FROM \"{table_name}\"', conn)\n",
    "    csv_path = os.path.join(output_dir, f\"{table_name}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "# Cleanup\n",
    "conn.close()\n",
    "print(\"All tables exported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./data/tabular/dev_databases/financial/financial.sqlite ./data/tabular/financial.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data/tabular/dev_databases\n",
    "!rm -r ./data/tabular/__MACOSX/*\n",
    "!rm -r ./data/tabular/__MACOSX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Test Tabular route within Redbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now that the csv files are downloaded, upload them into Redbox via the UI and execute the questions within the financial_dataset_original.json. Save the SQL statements into the financial_dataset_results.json.\n",
    "Do not upload the trans table as it is too big (3Millions rows). We will test questions that do not involve querying this table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Once finished, delete the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data/tabular/csv_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Compare results against ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Read evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/tabular/financial_dataset_results.json\") as f:\n",
    "    eval_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "select a record from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = eval_data[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "row[\"SQL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Check Ground truth answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "import sqlite3\n",
    "\n",
    "db_path = \"./data/tabular/financial.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(row[\"SQL\"])\n",
    "results = cursor.fetchall()\n",
    "conn.close()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Download generated sqlite database from the docker container. For this, you need to change processes.py to disable the deletion of the database file after each query. The code line need to be commented out: state = delete_db_file_if_exists(state) \n",
    "- Get the name of the local db file generated by tabular route: \n",
    "1. docker exec -it redbox-django-app-1 bash\n",
    "\n",
    "2. find . -name *.db\n",
    "- Download db file from the docker container to your local host\n",
    "\n",
    "3. docker cp redbox-django-app-1:/usr/src/app/<name_local_db_file>.db ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Check the answer when no evidence (external knowledge) is supplied. In this case, the prompt is the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "db_path = \"./data/tabular/generated_db_a2df5245-db22-4872-911c-6564340f9027.db\"  # replace the name of the local db here\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# check results of the SQL query when evidence is not defined in the prompt\n",
    "# in this case, the prompt is the question\n",
    "cursor.execute(row[\"SQL_redbox_without_evidence\"])\n",
    "results = cursor.fetchall()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Check the answer when evidence (external knowledge) is supplied. In this case, the prompt is the question + evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results of the SQL query when evidence is added to the prompt\n",
    "# in this case, the prompt is the question + evidence\n",
    "\n",
    "cursor.execute(row[\"SQL_redbox_with_evidence\"])\n",
    "results = cursor.fetchall()\n",
    "conn.close()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "then record the accuracy in the financial_dataset_results.json. \n",
    "- is_accurate is 0 if the results from redbox does not match ground truth, otherwise it is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_list = []\n",
    "for row in eval_data:\n",
    "    if row[\"evidence\"] != \"\":\n",
    "        accuracy_with_evidence_list.append(row[\"is_accurate_with_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_without_evidence_list = []\n",
    "for row in eval_data:\n",
    "    accuracy_without_evidence_list.append(row[\"is_accurate_without_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_without_evidence = sum(accuracy_without_evidence_list) / len(accuracy_without_evidence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy when evidence (external knowledge) is not defined in the prompt\n",
    "accuracy_without_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence = sum(accuracy_with_evidence_list) / len(accuracy_with_evidence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy when evidence (external knowledge) is in the prompt\n",
    "accuracy_with_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_notchallenging_list = []\n",
    "for row in eval_data:\n",
    "    if row[\"evidence\"] != \"\" and row[\"difficulty\"] != \"challenging\":\n",
    "        accuracy_with_evidence_notchallenging_list.append(row[\"is_accurate_with_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_notchallenging = sum(accuracy_with_evidence_notchallenging_list) / len(\n",
    "    accuracy_with_evidence_notchallenging_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy when evidence (external knowledge) is in the prompt excluding challenging questions\n",
    "accuracy_with_evidence_notchallenging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Final Clean-up : Delete database files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "delete database file of evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./data/tabular/financial.sqlite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "delete local database file created by tabular agent. Use the following command and replace the name of the database:\n",
    "- !rm -r ./data/tabular/name_local_db_file>.db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-app-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
