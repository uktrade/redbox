{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redbox RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REDBOX-204: [SPIKE] Evaluate DeepEval as the LLM evaluation framework for Redbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Overview](#first-section)\n",
    "* [Generate Evaluation Dataset](#second-section)\n",
    "* [Get responses from Redbox RAG endpoint](#third-section)\n",
    "* [Run E2E Evaluation Metrics](#fourth-section)\n",
    "* [Develop Custom Metrics](#fifth-section)\n",
    "* [How to add to CI-CD pipeline](#sixth-section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"first-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook experiments with how we could use the DeepEval framework for both LLM/RAG unit testing (CI/CD) and RAG evalution in Redbox, and aims to get the user a little more familiar with the DeepEval framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a spike, for now, do not mess with poetry set up and install deepeval into a fresh virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. run the terminal commands below to:\n",
    "\n",
    "- Deactivate the main project virtual environment\n",
    "- Create and use a separate virtual environment for running this spike notebook\n",
    "\n",
    "`source deactivate`\n",
    "\n",
    "`cd notebooks/evaluation`\n",
    "\n",
    "`pyenv virtualenv 3.11.8 eval`\n",
    "\n",
    "`pyenv shell eval`\n",
    "\n",
    "Restart vs code for eval virtualenv to be available as a kernel to run this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Evaluation Dataset <a class=\"anchor\" id=\"second-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evalution our RAG application end-to-end. In order to do this we need to:\n",
    "1. Generate a dataset from some of the documents I have access to\n",
    "    - Try using DeepEval synthesizer for this (currently does not create expected_output)\n",
    "    - *We can also use this [Hugging Face notebook](https://huggingface.co/learn/cookbook/en/rag_evaluation) to generate Q&A data and/or generated the expected_output (not done in this spike)*\n",
    "    - Put the document(s) and all synthetically generated questions through the e2e Redbox `/rag` endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepEval Synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use document(s) that we want to RAG over to generate Q&A pairs with relevant context - start simple with one doc.\n",
    "\n",
    "Document used in this example is: Compass_BassicIncomeForAll_2019.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps below to get the chunks for the evaluation document(s):\n",
    "\n",
    "1. Run app locally WITHOUT detached mode: `docker compose up elasticsearch kibana worker minio redis core-api db django-app`\n",
    "\n",
    "2. View Swagger UI for /file endpoint at: `http://127.0.0.1:5002/file/docs`\n",
    "\n",
    "3. Upload documents selected for evaluation\n",
    "\n",
    "4. Take a note of the uuid(s), e.g. 7b550232-35c4-48fd-8d7a-ba364c1378c4 (this will change each time you run locally)\n",
    "\n",
    "Chunking happens very quickly. Embedding takes more time, but will give you a boolean flag on complete.\n",
    "\n",
    "5. From the Swagger UI, use the `file/{uuid}/status` endpoint to check status. Use the `uuid`s noted in step 4\n",
    "\n",
    "6. From the Swagger UI use the `{file_uuid}/chunks` endpoint to get the chunks required for the next step of evaluation. Use the `uuid`s noted in step 4 to get chunks required for evaluation.\n",
    "\n",
    "The complete output can be downloaded in JSON format from the Swagger UI docs page\n",
    "\n",
    "7. Move downloaded response into `notebooks/evaluation/data_eval` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use [From Contexts](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data#from-contexts) method in DeepEval synthesizer - this will ensure a more robust evaluation, with actual Redbox chunking mechanism used**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_goldens` method within the Synthesizer class allows for the creation of an evaluation dataset from a manually provided list of `contexts`, which is of type `list[list[str]]`.\n",
    "\n",
    "This method directly transforms predefined textual contexts into inputs, which are then evolved. The evolved inputs form the basis of the goldens in your evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunks created by Redbox\n",
    "import json\n",
    "\n",
    "# Define the path to the JSON file\n",
    "file_path = \"data_eval/response_1715091628456.json\"\n",
    "\n",
    "# Open the file and load the JSON data\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of contexts for synthetic data generation, by taking the text of each chunk from the JSON response\n",
    "contexts = []\n",
    "for i in range(len(data)):\n",
    "    contexts.append([data[i][\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# Initialize the Synthesizer\n",
    "synthesizer = Synthesizer()\n",
    "\n",
    "# contexts generated in cell above\n",
    "\n",
    "# Generate goldens directly with the synthesizer\n",
    "synthesizer.generate_goldens(contexts=contexts)\n",
    "synthesizer.save_as(\n",
    "    file_type=\"json\",  # The method also supports 'csv'\n",
    "    directory=\"./synthetic_data\",\n",
    ")\n",
    "\n",
    "# Generate goldens within an EvaluationDataset\n",
    "dataset = EvaluationDataset()\n",
    "dataset.generate_goldens(synthesizer=synthesizer, contexts=contexts)\n",
    "dataset.save_as(\n",
    "    file_type=\"json\",  # Similarly, this supports 'csv'\n",
    "    # directory=\"./synthetic_data\"\n",
    "    directory=\"./data_eval/synthetic_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generate_goldens method in deepeval, the parameters are:\n",
    "\n",
    "- contexts: a list of contexts, where each context is itself a list of strings sharing a common theme or subject area.\n",
    "- [Optional] max_goldens_per_context: the maximum number of golden data points to be generated from each context. Adjusting this parameter can influence the size of the resulting dataset. Defaulted to 2.\n",
    "- [Optional] num_evolutions: the number of evolution steps to apply to each generated input. This parameter controls the complexity and diversity of the generated dataset by iteratively refining and evolving the initial inputs. Default value is 1.\n",
    "- [Optional] enable_breadth_evolve: a boolean indicating whether to enable breadth evolution strategies during data generation. When set to True, it introduces a wider variety of context modifications, enhancing the dataset's diversity. Default value is False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review synthetically created dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunks created by Redbox\n",
    "import json\n",
    "\n",
    "# Define the path to the JSON file\n",
    "file_path = \"data_eval/synthetic_data/20240507_173642.json\"\n",
    "\n",
    "# Open the file and load the JSON data\n",
    "with open(file_path, \"r\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goldens do not have `expected_output` as it is not required or all metrics. These need to be generated when you create evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"input\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create An Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `EvaluationDataset` in `deepeval` is simply a collection of `LLMTestCases` and/or `Goldens`.\n",
    "\n",
    "**INFO**\n",
    "\n",
    "A `Golden` is extremely very similar to an `LLMTestCase`, but they are more flexible as they do not require an `actual_output` at initialization. On the flip side, whilst test cases are always ready for evaluation, a golden isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Test_Cases (come back to this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Goldens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should opt to initialize `EvaluationDatasets` with goldens if you're looking to generate LLM outputs at evaluation time. This usually means your original dataset does not contain precomputed outputs, but only the inputs you want to evaluate your LLM (application) on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This IS the case for us**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "first_golden = Golden(input=dataset[0][\"input\"])\n",
    "\n",
    "goldens = [first_golden]\n",
    "\n",
    "test_dataset = EvaluationDataset(goldens=goldens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of May 2024, `DeepEval`'s synthesizer only generated `input`, i.e. questions. It does not created `expected_output` - this will be coming in the next release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Additionally, the synthesizer (I think) only generates the `input` from a single chunk. A `#TODO` would be to create `input` questions from multiple combinations of contexts, to generate more complex `input` questions that require more than one chunk to be retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to also collect the context used to generate the `input` with the real context returned by Redbox Core API. Some metrics may require the context used by the synthesizer, so best to keep all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****TODO:** Generate expected output using LLMs ([HuggingFace have a good notebook on this](https://huggingface.co/learn/cookbook/en/rag_evaluation))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate expected output for the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get responses from Redbox RAG endpoint <a class=\"anchor\" id=\"third-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload document(s) that we want to RAG over - start simple with one doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation docs should already have been uploaded to the locally running application, in the steps above. If not, do so now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format each question into the required schema for the /rag endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dict = {\n",
    "        \"message_history\": [\n",
    "            {\"role\": \"system\", \"text\": \"You are a helpful AI Assistant\"},\n",
    "            {\"role\": \"user\", \"text\": dataset[i][\"input\"]},\n",
    "        ]\n",
    "    }\n",
    "    payloads.append(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling /chat/rag endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python's concurrent.futures module to achieve parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will send POST requests to the specified FastAPI endpoint (/chat/rag) with the provided JSON payload. The ThreadPoolExecutor is used to send these requests in parallel, with a maximum of 10 workers. The status code of each request is printed to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle the responses in a more efficiently by storing them in a list and processing them after all requests have been made. This way, you can perform operations like counting the number of successful requests, logging failed requests, etc. Here's how you can modify your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def post_request(payload):\n",
    "    url = \"http://127.0.0.1:5002/chat/rag\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "    return response.status_code, response.json()  # return status code and response body\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(post_request, payload) for payload in payloads]\n",
    "\n",
    "responses = [future.result() for future in futures]  # store responses in a list\n",
    "\n",
    "# Create a list to store status and body\n",
    "status_and_body = [(status, body) for status, body in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_and_body[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save respones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the status_and_body list to a file\n",
    "with open(\"data_eval/rag_responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(status_and_body, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the status_and_body list from a file\n",
    "with open(\"data_eval/rag_responses.pkl\", \"rb\") as f:\n",
    "    status_and_body = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_and_body[0][1][\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_and_body[i][1][\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = [d[\"page_content\"] for d in status_and_body[0][1][\"source_documents\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i][\"retrieved_context\"] = status_and_body[i][1][\"source_documents\"][\"page_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i][\"retrieved_context\"] = status_and_body[i][1][\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i][\"actual_output\"] = status_and_body[i][1][\"output_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Retrieval <a class=\"anchor\" id=\"first-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which context should we to use (the one used for generating sythetic data or the returned context from Redbox /chat/rag) --> You should use the context returned by your RAG application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualPrecisionMetric, ContextualRecallMetric, ContextualRelevancyMetric\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric()\n",
    "contextual_recall = ContextualRecallMetric()\n",
    "contextual_relevancy = ContextualRelevancyMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define a test case. Note that deepeval gives you the flexibility to either begin evaluating with complete datasets, or perform the retrieval and generation at evaluation time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "...\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=dataset[0][\"input\"],\n",
    "    actual_output=dataset[0][\"actual_output\"],\n",
    "    # TODO: need expected output\n",
    "    retrieval_context=page_contents,  # Needs to be None or a list of strings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "\n",
    "...\n",
    "\n",
    "evaluate(\n",
    "    test_cases=[test_case],\n",
    "    metrics=[contextual_relevancy],\n",
    "    # metrics=[contextual_precision, contextual_recall, contextual_relevancy]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Generation <a class=\"anchor\" id=\"first-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation metrics are included in the E2E run below and include RAGASFaithfulnessMetric & RAGASAnswerRelevancyMetric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
