{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Calculate execution accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this notebook, we will calculate the accuracy of tabular agent by comparing its results (resulting from SQL query) against the ground truth results. \n",
    "In the evaluation dataset, the questions and evidence are supplied. The evidence represents the external knowledge supplied to the LLM which can be related to the knowledge about data schema, definitons of columns or specific values, definitions of acronyms or specific function names for mathematical calculations. We will calculate the execution accuracy without evidence, and with evidence. This will allow us to estimate the impact of external knowledge on tabular agent performance. Also we will calculate the execution accuracy with evidence excluding challenging questions. This will allow us to get a more representative performance metric on less challenging use cases or less complex user questions. For more information about the execution accuracy, please check the BIRD paper: https://arxiv.org/pdf/2305.03111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "The baseline performance derived from the original paper (above) depends on the chosen model. When the paper was released (2023), the execution accuracy achieved by GPT4 was 30% without evidence and 46% with evidence. Please note that this is the accuracy achieved by GPT4 in 2023 and that the baseline performance would be higher if the model has been re-trained post 2023. More recent models combined with data profiling techniques and other approaches have achieved higher accuracy (75% on Dev set). The performance leaderbord can be accessed here: https://bird-bench.github.io. Please note that the Dev set is a much larger dataset, containing multiple databases (including the Financial dataset used for evaluation here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data_results/tabular/evaluation_results.json\") as f:\n",
    "    eval_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_list = []\n",
    "for row in eval_data:\n",
    "    if row[\"evidence\"] != \"\":\n",
    "        accuracy_with_evidence_list.append(row[\"is_accurate_with_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_without_evidence_list = []\n",
    "for row in eval_data:\n",
    "    accuracy_without_evidence_list.append(row[\"is_accurate_without_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_without_evidence = sum(accuracy_without_evidence_list) / len(accuracy_without_evidence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence = sum(accuracy_with_evidence_list) / len(accuracy_with_evidence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"execution accuracy without evidence: \", accuracy_without_evidence)\n",
    "print(\"execution accuracy with evidence:\", accuracy_with_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_notchallenging_list = []\n",
    "for row in eval_data:\n",
    "    if row[\"evidence\"] != \"\" and row[\"difficulty\"] != \"challenging\":\n",
    "        accuracy_with_evidence_notchallenging_list.append(row[\"is_accurate_with_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_notchallenging = sum(accuracy_with_evidence_notchallenging_list) / len(\n",
    "    accuracy_with_evidence_notchallenging_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy when evidence (external knowledge) is in the prompt excluding challenging questions\n",
    "print(\"execution accuracy with evidence excluding challenging questions: \", accuracy_with_evidence_notchallenging)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-app-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
