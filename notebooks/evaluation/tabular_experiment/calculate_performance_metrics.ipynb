{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11bbf9c",
   "metadata": {},
   "source": [
    "## Calculate execution accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51814755",
   "metadata": {},
   "source": [
    "In this notebook, we will calculate the accuracy of tabular agent by comparing its results (resulting from SQL query) against the ground truth results. \n",
    "In the evaluation dataset, the questions and evidence are supplied. The evidence represents the external knowledge supplied to the LLM which can be related to the knowledge about data schema, definitons of columns or specific values, definitions of acronyms or specific function names for mathematical calculations. We will calculate the execution accuracy without evidence, and with evidence. This will allow us to estimate the impact of external knowledge on tabular agent performance. Also we will calculate the execution accuracy with evidence excluding challenging questions. This will allow us to get a more representative performance metric on less challenging use cases or less complex user questions. For more information about the execution accuracy, please check the BIRD paper: https://arxiv.org/pdf/2305.03111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae8ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ccc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_results/tabular/evaluation_results.json') as f:\n",
    "    eval_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879e2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_list = []\n",
    "for row in eval_data:\n",
    "    if row[\"evidence\"] != \"\":\n",
    "        accuracy_with_evidence_list.append(row[\"is_accurate_with_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779dc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_without_evidence_list = []\n",
    "for row in eval_data:\n",
    "    accuracy_without_evidence_list.append(row[\"is_accurate_without_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed1d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_without_evidence=sum(accuracy_without_evidence_list)/len(accuracy_without_evidence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8777f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence=sum(accuracy_with_evidence_list)/len(accuracy_with_evidence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5df458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy without evidence:  0.23333333333333334\n",
      "execution accuracy with evidence: 0.39285714285714285\n"
     ]
    }
   ],
   "source": [
    "print(\"execution accuracy without evidence: \",accuracy_without_evidence) \n",
    "print(\"execution accuracy with evidence:\",accuracy_with_evidence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e8c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_notchallenging_list = []\n",
    "for row in eval_data:\n",
    "    if row[\"evidence\"] != \"\" and row[\"difficulty\"] != \"challenging\":\n",
    "        accuracy_with_evidence_notchallenging_list.append(row[\"is_accurate_with_evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "874a740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_with_evidence_notchallenging=sum(accuracy_with_evidence_notchallenging_list)/len(accuracy_with_evidence_notchallenging_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea1a2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy with evidence excluding challenging questions:  0.5\n"
     ]
    }
   ],
   "source": [
    "#accuracy when evidence (external knowledge) is in the prompt excluding challenging questions\n",
    "print(\"execution accuracy with evidence excluding challenging questions: \",accuracy_with_evidence_notchallenging) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-app-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
