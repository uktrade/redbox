from pydantic import BaseModel, Field

from redbox.chains.parser import BaseCumulativeTransformOutputParser, ClaudeParser
from redbox.models import prompts
from redbox.models.settings import ChatLLMBackend


class PromptVariable(BaseModel):
    """
    This data class for storing variables that are used in the prompt template.
    """

    task: bool = Field(description="task information", default=False)
    expected_output: bool = Field(description="expected output from a task", default=False)
    metadata: bool = Field(description="file metadata containing file name, description, and keywords", default=False)
    chat_history: bool = Field(description="user chat history", default=False)
    question: bool = Field(description="user question", default=False)
    document_filenames: bool = Field(description="user selected document file names", default=False)
    format_instructions: bool = Field(description="format instruction for structured output", default=False)
    previous_plan: bool = Field(description="previous plan generated by planner", default=False)
    user_feedback: bool = Field(description="user feedback prompt", default=False)
    agents_results: bool = Field(description="information from other agents", default=False)
    formatted_documents: bool = Field(description="", default=False)


class PromptConfig(BaseModel):
    system: str = Field(description="sytem prompt")
    question: str | None = Field(description="question prompt", default=None)
    format: str | None = Field(description="prompt used for format instruction", default=None)
    prompt_vars: PromptVariable | None = Field(description="variables that are used in the prompt", default=None)


class PromptConfigs(BaseModel):
    """
    This class is for storing prompt configs for all the agents.
    """

    planner_agent = PromptConfig(
        system=prompts.PLANNER_PROMPT_TOP,
        question=prompts.PLANNER_QUESTION_PROMPT,
        format=prompts.PLANNER_FORMAT_PROMPT,
        prompt_var=PromptVariable(
            chat_history=True, question=True, document_filenames=True, metadata=True, format_instruction=True
        ),
    )

    replanner_agent = PromptConfig(
        system=prompts.REPLAN_PROMPT,
        question=prompts.PLANNER_QUESTION_PROMPT,
        format=prompts.PLANNER_FORMAT_PROMPT,
        prompt_var=PromptVariable(
            previous_plan=True,
            user_feedback=True,
            question=True,
            document_filenames=True,
            metadata=True,
            format_instruction=True,
        ),
    )

    internal_retrieval_agent = PromptConfig(
        system=prompts.INTERNAL_RETRIEVAL_AGENT_PROMPT,
        question=None,
        format=None,
        prompt_var=PromptVariable(task=True, expected_output=True, metadata=True),
    )

    summarisation_agent = PromptConfig(
        system=prompts.CHAT_WITH_DOCS_SYSTEM_PROMPT,
        question=prompts.CHAT_WITH_DOCS_QUESTION_PROMPT,
        format=None,
        prompt_var=PromptVariable(question=True, formatted_documents=True),
    )

    evaluator_agent = PromptConfig(
        system=prompts.NEW_ROUTE_RETRIEVAL_SYSTEM_PROMPT,
        question=prompts.NEW_ROUTE_RETRIEVAL_QUESTION_PROMPT,
        format=prompts.CITATION_PROMPT,
        prompt_var=PromptVariable(agents_results=True, question=True, format_instructions=True),
    )


class AgentConfig(BaseModel):
    name: str = Field(description="Name of agent")
    prompt: PromptConfig = Field(description="Prompts used for this agent")
    tools: list | None = Field(description="A set of tools available for this agent", default=None)
    max_tokens: int = Field(
        description="Maximum tokens for this agent. Response exceed this limit will be truncated.", default=5000
    )
    llm_backend: ChatLLMBackend
    parser: BaseCumulativeTransformOutputParser
    use_metadata: bool
    use_chat_history: bool


def get_worker_agent_config(
    name: str, prompt: PromptConfig, tools: list | None, use_metadata, use_chat_history
) -> AgentConfig:
    return AgentConfig(
        name=name,
        prompt=prompt,
        tools=tools,
        llm_backend=ChatLLMBackend(name="sonnet-4", provider="bedrock"),
        parser=ClaudeParser,
        use_metadata=use_metadata,
        use_chat_history=use_chat_history,
    )


class AgentConfigs(BaseModel):
    planner_agent = AgentConfig(
        name="planner_agent",
        prompt=PromptConfigs.planner_agent,
        tools=[],
        llm_backend=ChatLLMBackend(name="sonnet-4", provider="bedrock"),
        parser=ClaudeParser,
        use_metadata=PromptConfigs.planner_agent.prompt_vars.metadata,
        use_chat_history=PromptConfigs.planner_agent.prompt_vars.chat_history,
    )

    internal_retrieval_agent = get_worker_agent_config(
        name="internal_retrieval_agent", prompt=PromptConfigs.internal_retrieval_agent, tools=[]
    )

    summarisation_agent = AgentConfig(
        name="summarisation_agent",
        prompt=PromptConfigs.summarisation_agent,
        tools=[],
        llm_backend=ChatLLMBackend(name="sonnet-4", provider="bedrock"),
        parser=ClaudeParser,
        use_metadata=PromptConfigs.summarisation_agent.prompt_vars.metadata,
        use_chat_history=PromptConfigs.summarisation_agent.prompt_vars.chat_history,
    )
